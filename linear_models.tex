\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Linear models}
\author{bartczukkuba }
\date{February 2020}

\begin{document}

\maketitle

\section{Simple linear regression}

Model

$$y_i = \beta_1 x_i + \beta_0 + \epsilon_i$$


Least squares estimation:

$b_1 = \frac{\sum x_i y_i}{\sum (x_i - \bar{x})^2}$

$b_0 = \bar{y} - b_1 \bar{x}$

Distribution of estimators

$b_1 \sim \mathcal{N}(\beta_1, \frac{\sigma^2}{\sum (x_i - \bar{x})^2}) $

$b_0 \sim \mathcal{N}(\beta_0, \sigma^2(\frac{1}{n} + \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2})) $

$\hat{y_i} = b_0 + b_1 x_i $

So $\hat{y_i} \sim \mathcal{N}(0, \sigma^2(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum (x_i - \bar{x})^2}))$

\subsection{Inference}

\textbf{Some definitions}

$SSM = \sum(\hat{y_i} - \bar{y})^2$

$SSE = \sum(y_i - \hat{y_i})^2$

$SST = \sum(y_i - \bar{y})^2$

$SST = SSM + SSE$


$$\frac{\beta_1 - b_1}{s[b_1]}$$ follows Student's $t$-distribution with $n-2$ degrees of freedom (one substituted per intercept, and one per slope)

\textbf{Hypothesis testing}


$H_0 : \beta_1 = 0$ vs $H_0 : \beta_1 \neq 0$

Under $H_0$, $T= \frac{b_1}{s[b_1]}$ follows Student's $t$ distribution.

Thus, if $T > t(1-\frac{\alpha}{2}, df)$ then $T$ is less than $\alpha$ -probable.


\section{Multiple linear regression}

Model
 
$y = X \beta + \epsilon$

Where

$X$ - $n \times p$ design matrix

$\beta$ - $p$-dimensional vector of parameters

$\epsilon \sim \mathcal{N}(0, \sigma^2 \mathbb{I})$

\subsection{Estimation}

Least squares estimator is given by

$b = (X^TX)^{-1}X^Ty$

$b \sim \mathcal{N}(\beta, \sigma^2 (X^T X)^{-1})$

$\hat{y} = Xb$ so $\hat{y} \sim \mathcal{N}(y, X(X^TX)^{-1}X^T)$ 

\subsection{Inference}

Since $X$ is multidimensional, T-test is not applicable for all the parameters.

We use F-test

$F = MSM/MSE = \frac{SSM/p-1}{SSE/n-1}$

Which has F-distribution $F(p-1, n-p)$ under null hypothesis

$H_0: \forall_i \beta_i = 0$, $H_1: \exists_i \beta_i \neq 0$

\section{Model selection}

As a goodness of fit oftentimes $R^2$ is used:

$$R^2 = \frac{SSM}{SST} = 1 - \frac{SSE}{SST}$$

It is also called coefficient of explained variance

In multiple linear regression adding new predictor variables will always increase $R^2$. Model selection criteria aim at providing single metric that will work for different $p$

$$AIC = n log(\frac{SSE_p}{n}) + 2p$$

$$BIC = n log(\frac{SSE_p}{n}) + p log(n)$$

$$C(p) = \frac{SSE_p}{MSE} - (n - 2p)$$

Adjusted $R^2$

$\hat{R^2} = 1 - (1-R^2) \frac{n-1}{n-p-1}$

Also

$\hat{R^2} = 1 - \frac{SS_{res}/df_e}{SS_{tot}/df_t}$

\section{Diagnostics}

\begin{itemize}
    \item (simple regression) plot $x$ vs $y$ - can see whether relation is linear
    \item plot $y - \hat{y}$ vs $y$ - can see whether prediction is biased, and variance is constant
    \item quantile-quantile plot - plot empirical quantiles against ones for normal distribution
\end{itemize}

\textbf{DFFITS}

Predict without $X_i$, $y_i$; test how prediction $\hat{y_i}$ changes

\textbf{DFBETAS}

Standardized difference of $\beta_j$ computed with and without $X_i$, $y_i$

\textbf{Cook's distance}

\section{Remedial measures}

Box-Cox transformation

Weighted Least Squares

\end{document}
